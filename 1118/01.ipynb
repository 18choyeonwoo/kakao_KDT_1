{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ba4213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Installing collected packages: joblib, nltk\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [nltk][32m1/2\u001b[0m [nltk]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.2 nltk-3.9.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b2c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk 데이터 경로 설정\n",
    "nltk.data.path.append(\"/home/ubuntu/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97bea4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk 데이터 다운로드\n",
    "nltk.download('punkt', download_dir=\"/home/ubuntu/nltk_data\")\n",
    "nltk.download('punkt_tab', download_dir=\"/home/ubuntu/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44332ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!', 'this', 'is', 'a', 'test', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Hello, world! this is a test sentence\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d08bb8",
   "metadata": {},
   "source": [
    "Konlpy 토큰화 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3acb2dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting lxml>=4.1.0 (from konlpy)\n",
      "  Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from konlpy) (2.2.6)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
      "Downloading lxml-6.0.2-cp312-cp312-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml, JPype1, konlpy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [konlpy]2m1/3\u001b[0m [JPype1]\n",
      "\u001b[1A\u001b[2KSuccessfully installed JPype1-1.6.0 konlpy-0.6.0 lxml-6.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148f6ee",
   "metadata": {},
   "source": [
    "sudo apt update\n",
    "sudo apt-get install openjdk-11-jdk \n",
    "java --version\n",
    "\n",
    "java_home 환경변수 설정\n",
    "export JAVA_HOME=/usr/lib/jvm/java-11-open\n",
    "자바가 설치된 경로를 알랴줌\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "그 경로를 시스템이 바로 쓸 수 있게 해쥼  (일단 안했음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf6439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d6e47b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', '반갑습니다', '저', '는', '공부', '하기', '싫어요', '헬로']\n"
     ]
    }
   ],
   "source": [
    "korean_sentence = \"안녕하세요 반갑습니다 저는 공부하기 싫어요 헬로\"\n",
    "korean_tokens = okt.morphs(korean_sentence)\n",
    "print(korean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dad9b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = [\n",
    "    \"영화 추천 해줘.\",\n",
    "    \"내 mbti는 뭐야?\",\n",
    "    \"내 최애 영화는 라라랜드야.\",\n",
    "    \"안녕하세요 저는 김민수 입니다.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "926bf9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_intent(tokens):\n",
    "    greetings = [\"안녕\", \"안녕하세요\", \"하이\", \"헬로\"]\n",
    "    farewells = [\"잘 가\", \"안녕히 가세요\", \"바이\", \"굿바이\"]\n",
    "    time_queries = [\"시간\", \"몇 시\", \"현재 시각\"]\n",
    "    weather_queries = [\"날씨\", \"기상\", \"오늘 날씨\"]\n",
    "    help_requests = [\"도와줘\", \"도와줄 수 있어\", \"도움 필요해\"]\n",
    "    math_queries = [\"더하기\", \"빼기\", \"곱하기\", \"나누기\"]\n",
    "\n",
    "    token_set = set(tokens)\n",
    "\n",
    "    if token_set.intersection(greetings):\n",
    "        return \"인사\"\n",
    "    elif token_set.intersection(farewells):\n",
    "        return \"작별 인사\"\n",
    "    elif token_set.intersection(time_queries):\n",
    "        return \"시간 문의\"\n",
    "    elif token_set.intersection(weather_queries):\n",
    "        return \"날씨 문의\"\n",
    "    elif token_set.intersection(help_requests):\n",
    "        return \"도움 요청\"\n",
    "    elif token_set.intersection(math_queries):\n",
    "        return \"수학 질문\"\n",
    "    else:\n",
    "        return \"알 수 없음\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3d69dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장: 영화 추천 해줘. -> 토큰: ['영화', '추천', '해줘', '.']\n",
      "의도: 알 수 없음\n",
      "\n",
      "문장: 내 mbti는 뭐야? -> 토큰: ['내', 'mbti', '는', '뭐', '야', '?']\n",
      "의도: 알 수 없음\n",
      "\n",
      "문장: 내 최애 영화는 라라랜드야. -> 토큰: ['내', '최애', '영화', '는', '라라', '랜드', '야', '.']\n",
      "의도: 알 수 없음\n",
      "\n",
      "문장: 안녕하세요 저는 김민수 입니다. -> 토큰: ['안녕하세요', '저', '는', '김민수', '입니다', '.']\n",
      "의도: 인사\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for s in test_sentence:\n",
    "    tokens = okt.morphs(s)\n",
    "    print(f\"문장: {s} -> 토큰: {tokens}\")\n",
    "    print(f\"의도: {classify_intent(tokens)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8889e816",
   "metadata": {},
   "source": [
    "의도 분류 학습, 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "472ce18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (0.6.0)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from konlpy) (1.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from konlpy) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.6 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from konlpy) (2.2.6)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from tensorflow) (4.15.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.10.5)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (12.0.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ubuntu/flask_app/venv/lib/python3.12/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached grpcio-1.76.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "Downloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading wrapt-2.0.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, threadpoolctl, termcolor, tensorboard-data-server, protobuf, optree, opt_einsum, ml_dtypes, mdurl, markdown, h5py, grpcio, google_pasta, gast, absl-py, tensorboard, scikit-learn, markdown-it-py, astunparse, rich, keras, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/26\u001b[0m [tensorflow]6\u001b[0m [tensorflow]-py]a-server]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 keras-3.12.0 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.4 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 protobuf-6.33.1 rich-14.2.0 scikit-learn-1.7.2 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 threadpoolctl-3.6.0 wheel-0.45.1 wrapt-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3622bbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 02:33:58.543445: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1eb9e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "   \"안녕하세요\", \"오늘 날씨 어때요?\", \"문 좀 열어줘\", \"몇 시야?\", \"안녕 반가워요\",\n",
    "   \"음악 틀어줘\", \"내일 일정 알려줘\", \"밖에 비 와?\", \"도움 필요해\", \"영화 추천해줘\"\n",
    "]\n",
    "labels = [\n",
    "   \"인사\", \"질문\", \"명령\", \"질문\", \"인사\",\n",
    "   \"명령\", \"질문\", \"질문\", \"명령\", \"명령\"\n",
    "]\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "tokenized = [okt.morphs(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4e6e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 단어 사전(집합) 만들기\n",
    "vocab = {}\n",
    "for sent in tokenized:\n",
    "   for word in sent:\n",
    "       if word not in vocab:\n",
    "           vocab[word] = len(vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6848440c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 04:43:10.292336: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 1s - 1s/step - accuracy: 0.2500 - loss: 1.0991\n",
      "Epoch 2/10\n",
      "1/1 - 0s - 24ms/step - accuracy: 0.6250 - loss: 1.0851\n",
      "Epoch 3/10\n",
      "1/1 - 0s - 24ms/step - accuracy: 0.7500 - loss: 1.0732\n",
      "Epoch 4/10\n",
      "1/1 - 0s - 24ms/step - accuracy: 0.8750 - loss: 1.0625\n",
      "Epoch 5/10\n",
      "1/1 - 0s - 24ms/step - accuracy: 0.8750 - loss: 1.0519\n",
      "Epoch 6/10\n",
      "1/1 - 0s - 24ms/step - accuracy: 0.8750 - loss: 1.0415\n",
      "Epoch 7/10\n",
      "1/1 - 0s - 24ms/step - accuracy: 0.8750 - loss: 1.0310\n",
      "Epoch 8/10\n",
      "1/1 - 0s - 24ms/step - accuracy: 0.8750 - loss: 1.0197\n",
      "Epoch 9/10\n",
      "1/1 - 0s - 24ms/step - accuracy: 0.8750 - loss: 1.0087\n",
      "Epoch 10/10\n",
      "1/1 - 0s - 24ms/step - accuracy: 0.8750 - loss: 0.9970\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "문장: 안녕하세요!\n",
      "→ 예측 의도: 인사\n",
      "\n",
      "문장: 문 좀 열어줘.\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 현재 시각이 궁금해.\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 창문을 닫아.\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 오늘 날씨 어때?\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 도와줄 수 있어?\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 잘 지내?\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 동영상 틀어줘.\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 2 더하기 2는 뭐야?\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 영화 추천해줘.\n",
      "→ 예측 의도: 명령\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 임베딩을 위한 인덱스 변환 (벡터화)\n",
    "encoded_sentences = [[vocab[word] for word in sent] for sent in tokenized]\n",
    "\n",
    "\n",
    "# 신경망 입력 크기 통일\n",
    "max_len = max(len(seq) for seq in encoded_sentences)\n",
    "padded_sentences = pad_sequences(encoded_sentences, maxlen=max_len, padding='post')\n",
    "\n",
    "# 레이블 인코딩 후 원-핫 인코딩\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(labels)\n",
    "categorical_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# 학습 및 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sentences, categorical_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# 모델 정의\n",
    "embedding_dim = 50\n",
    "model = Sequential([\n",
    "   Embedding(len(vocab) + 1, embedding_dim),\n",
    "   Conv1D(64, 3, activation='relu'),\n",
    "   GlobalMaxPooling1D(),\n",
    "   Dense(32, activation='relu'),\n",
    "   Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 모델 훈련\n",
    "model.fit(X_train, y_train, epochs=10, verbose=2)\n",
    "\n",
    "# 테스트 추론 및 결과 출력\n",
    "test_sentences = [\n",
    "   \"안녕하세요!\", \"문 좀 열어줘.\", \"현재 시각이 궁금해.\", \"창문을 닫아.\",\n",
    "   \"오늘 날씨 어때?\", \"도와줄 수 있어?\", \"잘 지내?\", \"동영상 틀어줘.\",\n",
    "   \"2 더하기 2는 뭐야?\", \"영화 추천해줘.\"\n",
    "]\n",
    "\n",
    "\n",
    "test_tokenized = [okt.morphs(s) for s in test_sentences]\n",
    "test_encoded = [[vocab.get(word, 0) for word in sent] for sent in test_tokenized] \n",
    "test_padded = pad_sequences(test_encoded, maxlen=max_len, padding='post')\n",
    "\n",
    "\n",
    "\n",
    "pred_probs = model.predict(test_padded)\n",
    "pred_labels = le.inverse_transform(np.argmax(pred_probs, axis=1))\n",
    "\n",
    "\n",
    "for sent, label in zip(test_sentences, pred_labels):\n",
    "   print(f\"문장: {sent}\\n→ 예측 의도: {label}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20d4bb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text_classification_cnn_model.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 모델 저장(joblib 사용)\n",
    "import joblib\n",
    "joblib.dump(model, 'text_classification_cnn_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c56b1aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "문장: 안녕, 오늘 기분 어때?\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 내일 비 올까?\n",
      "→ 예측 의도: 질문\n",
      "\n",
      "문장: 음악 재생해줘.\n",
      "→ 예측 의도: 명령\n",
      "\n",
      "문장: 시간 알려줘.\n",
      "→ 예측 의도: 명령\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 불러오기 및 추론활용\n",
    "import joblib\n",
    "model = joblib.load('text_classification_cnn_model.pkl')\n",
    "\n",
    "\n",
    "new_sentences = [\n",
    "   \"안녕, 오늘 기분 어때?\",\n",
    "   \"내일 비 올까?\",\n",
    "   \"음악 재생해줘.\",\n",
    "   \"시간 알려줘.\"\n",
    "]\n",
    "new_tokenized = [okt.morphs(s) for s in new_sentences]\n",
    "new_encoded = [[vocab.get(word, 0) for word in sent] for sent in new_tokenized]\n",
    "new_padded = pad_sequences(new_encoded, maxlen=max_len, padding='post')\n",
    "\n",
    "\n",
    "new_pred_probs = model.predict(new_padded)\n",
    "\n",
    "\n",
    "new_pred_labels = le.inverse_transform(np.argmax(new_pred_probs, axis=1))\n",
    "\n",
    "\n",
    "for sent, label in zip(new_sentences, new_pred_labels):\n",
    "   print(f\"문장: {sent}\\n→ 예측 의도: {label}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
